% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
%
\usepackage{graphicx}
\graphicspath{ {images/} }
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads

\mainmatter              % start of the contributions
%
\title{Anomaly Detection Challenges - Challenge II}
%
\titlerunning{Challenge 1}  % abbreviated title (for running head)
%                                     also used for the TOC unless

%
\author{Hamza Tahir (03670002) \and Muhammad Hamza Usmani (03669506)}
%
\authorrunning{Tahir \and Usmani} % abbreviated author list (for running head)
%
\institute{Technical University of Munich}
%%%% list of authors for the TOC (use if author list has to be modified)

\maketitle              % typeset the title of the contribution


%
\section{Introduction}
%
This brief report serves as a purpose to present and explain the methodologies applied to tackle the second challenge in the Practical: Anomaly Detection Challenges. Section~2 discusses the challenge task and the data set for the machine learning/anomaly detection task. Section~3 explains the approaches adopted for the task. Finally, Section~4 summarizes the results.
%
\section{The Challenge}
%
%
\subsubsection{Machine Learning Task}
%
The machine learning task for this challenge is to determine if hotel review from Yelp dataset is 'fake' or not 'fake'
\begin{table}
\caption{Decision Classes}
\begin{center}
\begin{tabular}{r@{\quad}rl}
\hline
\multicolumn{1}{l}{\rule{2pt}{0pt}
Class}&\multicolumn{2}{l}{Representation}\\[2pt]
\hline\rule{0pt}{12pt}
Fake Review&    Y& \\
Genuine Review&     N& \\[2pt]
\hline
\end{tabular}
\end{center}
\end{table}
%
\subsubsection{The Data set}
The dataset consists of hotel reviews from Yelp data set. The reviews are hotel reviews from Illinois Chicago area. The training data set of the challenge consists of 2969 reviews. The training data set has uneven class distribution, there are 2319 genuine reviews while there are 377 fake samples. The test data has 2950 reviews.

Hotel data about all reviews is also available, while almost all data about reviewers is also part of the data set, the data of 4 reviewers in the training set and 9 reviewers in test set is however missing.

\section{Methodology}
This section explains the data analysis and the machine learning process to build the model for classifying the given samples as fake  or not fake reviews. 
\subsection{Features - Linguistic or Behavioral?}
For classification of reviews, there are generally two types of features that are used \cite{arjun:et}: Linguistic or Behavioral. The former focuses on the content of the reviews themselves, generally using a n-gram feature approach, while the latter is focused on the 'meta-data' of the review, such as information about the reviewer etc. 

Even though linguistic features have been shown to get an accuracy of up to 90\% \cite{ott2011finding}, linguistic features are at times not useful in finding if a review is fake or not,  \cite{arjun:et}. Behavioral features however are generally more robust and helpful with classification of reviews as fake or not. With this in mind, we opted to go for a behavioral spam analysis approach rather than a linguistic one. 

Following is a list of features we used through our experiments. Some of them are taken straight from suggestions by \cite{arjun:et}. Table 2 also summarizes the features used for our analysis.
%
\subsubsection{Maximum Number of Reviews (MNR):}
%
Maximum number of reviews (MNR), or maximum reviews per reviewer per day are number of reviews of a reviewer in a day. According to \cite{arjun:et} , spammers have more reviews per day, in comparison to non-spammers. 
%
\subsubsection{Percentage of Positive Reviews (PR):}
%
Percentage Positive Reviews (PR) is the percentage of reviews of a reviewer that are positive. A review is considered to be positive by \cite{arjun:et} when it is rated 4+. Majority of spammers have most of their reviews as positive.
%
\subsubsection{Review Length (RL):}
%
Review Length (RL) is another behavioral feature considered by \cite{arjun:et} to distinguish between spammers and non-spammers. Majority of spammers have higher average review word length (>200), according to \cite{arjun:et}.
%
\subsubsection{Reviewer Deviation (RD):}
%
The spammers are likely to deviate from the general opinion \cite{arjun:et}.  The  reviewer deviation is the difference between a rating and the average rating of a hotel. According to \cite{arjun:et}; majority of non-spammers are bounded by an absolute deviation of 0.6.
%
\subsubsection{Review 'Metadata' (RWM):}
%
For each review, we had its 'rating', 'usefulCount', 'coolCount' and 'funnyCount'. According to our analysis, spam reviews generally had a low value for each of these features.
%
\subsubsection{Reviewer 'Metadata' (RRM):}
%
For each reviewer, we had his or her 'friendCount', 'firstCount', 'usefulCount', 'coolCount', 'funnyCount', 'complimentCount', 'tipCount' and 'fanCount'. Again, our analysis showed that the values for users who wrote 'fake' reviews were generally low across these features.
%
\begin{table}[h]
	\caption{Behavioral Features Comparison}
	\begin{tabular}{ |p{3cm}|p{4cm}| p{4cm}|}
		\hline
		Feature                        &Claim                  &In Given Dataset\\
		\hline
		Max Number of Reviews (MNR)     &Spammers have multiple reviews per day.  &Most reviewers post only once per day\\
		\hline
		Review Length (RL)  &Fake reviews have greater lengths.   &Average length of fake reviews is 122 while that of genuine one is 157 words.\\
		\hline
		Percent Positive Reviews (PR)   &Spammers generally rate more, in most of their reviews.   &Average of Percentage Positive Reviews of Spammers is: 0.52, while that of Non-spammers is 0.62.\\
		\hline
		Reviewer Deviation (RD)               &Spammers deviate more than general opinion.     &Spammers on average deviate by 1.17 stars, while genuine reviewers deviate on average by 0.85.\\
		\hline
		Review Metadata (RWM)               &Review spams have less 'counts; than normal.     &Our analysis shows that reviews that were marked fake generally had less 'usefulCounts', 'funnyCounts' etc.\\
		\hline
		Reviewer Metadata (RRM)               &Spammers have less 'counts' than normal.     &Our analysis shows that reviewers that made fake reviews generally had less 'usefulCounts', 'funnyCounts' etc.\\
		\hline
	\end{tabular}
	\label{table}
\end{table}
\subsection{Importing data to a relational database}
As the data we had was scattered across four different files, which had relationships amongst one another, we decided to pre-process the data and put it directly in a relational database. This was to make further analysis and feature extraction easier. We decided on a SQLite database, with four tables, namely: 'reviews\_test', 'reviews\_train', 'reviewer' and 'hotel'. 
\subsection{Feature Scaling}
The given features and the extended behavioral features have different scales, this required to normalize all features on one scale, so that the different ranges and scales of features do not contribute to relative weights of those features. To normalize, the following method was used:
\[ X^{'} =\frac{X - X_{min}}{X_{max} - X_{min}}   \]
\subsection{Re-sampling}
The training data set is higher number of non-fake to fake reviews, thus the data was re-sampled to build a robust classifier. The re-sampled data set had 377 genuine reviews that were randomly chosen from the data set, and 377 fake reviews from the training data set.

\subsection{Machine Learning Techniques}
Following machine learning techniques were used to classify given samples as "Genuine Review" aka. "Non-fake review" (represented as 0 or "N") or "Fake Review"(represented as 1 or "Y"):
\begin{enumerate}
   \item Naive Bayes
   \item Support Vector Classifier (SVC)
   \item Random Forest
 \end{enumerate}
 
\subsection{Generalizing - Developing a Robust Classifier}
To build a robust classifier, that is relatively general and is not restricted only to the given training set following techniques were used:

\subsubsection{Three-fold Cross Validation:} Three-fold cross validation was used to overcome the problem of over-fitting, and to build a model to that will generalize to an independent dataset, Hawkins et. al. (see \cite{hawkins:eke}).  


\section{Results}
Results of the challenge are summarized in this section. It must be noted that the Random Forest classier was trained without normalization (as it used entropy as the spiting measure).
The results shown here are the best accuracies we could get for each classifier. \\
Our analysis shows that the \textbf{Random Forest} approach works best. Our analysis also shows that training only on the metadata features (\textbf{RRM} and \textbf{RWM}) yeild the best results. Therefore, we ignored the rest of the features (suggested by \cite{arjun:et}). This could be because the data we had was particularly limited and had only a few thousand reviews. Therefore the extended features in \cite{arjun:et} were limiting as there was not enough data for them to be discriminatory.

\begin{table}[h]
\centering
  \caption{Training and Testing Results }
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}| }
 \hline
 Technique&Training Accuracy    &Test Accuracy &Average\\
 \hline
 Naive Bayes                &87.402    &86.768 &\textbf{87.04}\\
 SVC                        &85.279    &87.103
  &\textbf{86.191}\\
 Random Forest              &88.589    &89.539 &\textbf{89.064}\\
 \hline
 Average     &\textbf{87.09}&\textbf{87.773} &\textbf{}\\
  \hline
\end{tabular}
\label{table}
\end{table}



%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {arjun:et}
Mukherjee A., Venkatarman V., Liu Bing and Glance N.:
What Yelp Fake Review Filter Might Be Doing?
Seventh International AAAI Conference Weblogs and Social Media

\bibitem {hawkins:eke}
Hawkins D. , Basak S. , and Denise M. 
Assessing Model Fit by Cross-Validation
J. Chem. Inf. Comput. Sci., 2003, 43 (2), pp 579–586 (2003)

\bibitem {ott2011finding}
Ott, Myle and Choi, Yejin and Cardie:
Finding deceptive opinion spam by any stretch of the imagination; 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, 2011, 309--319

\end{thebibliography}
\end{document}
